{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictions of Druggable peptides using the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatically created module for IPython interactive environment\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\Users\\Eric\\Miniconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "# remove warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import confusion_matrix,accuracy_score, roc_auc_score,f1_score, recall_score, precision_score\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression, LassoCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "from sklearn.feature_selection import RFECV, VarianceThreshold, SelectKBest, chi2\n",
    "from sklearn.feature_selection import SelectFromModel, SelectPercentile, f_classif\n",
    "\n",
    "import seaborn as sns; sns.set() # data visualization library \n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier, BaggingClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB, GaussianNB\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score, KFold, StratifiedKFold\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "print(__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DataCheckings(df):\n",
    "    # CHECKINGS ***************************\n",
    "    # Check the number of data points in the data set\n",
    "    print(\"\\nData points =\", len(df))\n",
    "    \n",
    "    # Check the number of columns in the data set\n",
    "    print(\"\\nColumns (output + features)=\",len(df.columns))\n",
    "    \n",
    "    # Check the data types\n",
    "    print(\"\\nData types =\", df.dtypes.unique())\n",
    "    \n",
    "    # Dataset statistics\n",
    "    print('\\n')\n",
    "    df.describe()\n",
    "    \n",
    "    # print names of columns\n",
    "    print('Column Names:\\n', df.columns)\n",
    "    \n",
    "    # see if there are categorical data\n",
    "    print(\"\\nCategorical features:\", df.select_dtypes(include=['O']).columns.tolist())\n",
    "    \n",
    "    # Check NA values\n",
    "    # Check any number of columns with NaN\n",
    "    print(\"\\nColumns with NaN: \", df.isnull().any().sum(), ' / ', len(df.columns))\n",
    "\n",
    "    # Check any number of data points with NaN\n",
    "    print(\"\\nNo of data points with NaN:\", df.isnull().any(axis=1).sum(), ' / ', len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDataFromDataset(sFile, OutVar):\n",
    "    # read details file\n",
    "    print('\\n-> Read dataset', sFile)\n",
    "    df = pd.read_csv(sFile)\n",
    "    #df = feather.read_dataframe(sFile)\n",
    "    \n",
    "    DataCheckings(df)\n",
    "    \n",
    "    # remove duplicates!\n",
    "    df.drop_duplicates(keep=False, inplace=True)\n",
    "    \n",
    "    print('Shape', df.shape)\n",
    "    # print(list(df.columns))\n",
    "\n",
    "    # select X and Y\n",
    "    ds_y = df[OutVar]\n",
    "    ds_X = df.drop(OutVar,axis = 1)\n",
    "    Xdata = ds_X.values # get values of features\n",
    "    Ydata = ds_y.values # get output values\n",
    "\n",
    "    print('Shape X data:', Xdata.shape)\n",
    "    print('Shape Y data:',Ydata.shape)\n",
    "    \n",
    "    # return data for X and Y, feature names as list\n",
    "    return (Xdata, Ydata, list(ds_X.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def  set_weights(y_data, option='balanced'):\n",
    "    \"\"\"Estimate class weights for umbalanced dataset\n",
    "       If ‘balanced’, class weights will be given by n_samples / (n_classes * np.bincount(y)). \n",
    "       If a dictionary is given, keys are classes and values are corresponding class weights. \n",
    "       If None is given, the class weights will be uniform \"\"\"\n",
    "    cw = class_weight.compute_class_weight(option, np.unique(y_data), y_data)\n",
    "    w = {i:j for i,j in zip(np.unique(y_data), cw)}\n",
    "    return w "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define output variables\n",
    "outVar = 'Class'\n",
    "\n",
    "# define list of folds\n",
    "foldType = 3\n",
    "\n",
    "# define a label for output files\n",
    "targetName = 'GS_Outer'\n",
    "\n",
    "seed = 28"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reproduce the pipeline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-> Read dataset C:\\Users\\Eric\\Documents\\GitHub\\machine-learning-for-druggable-proteins\\datasets\\ds.Class_TC_ballanced.csv\n",
      "\n",
      "Data points = 1332\n",
      "\n",
      "Columns (output + features)= 8001\n",
      "\n",
      "Data types = [dtype('float64') dtype('int64')]\n",
      "\n",
      "\n",
      "Column Names:\n",
      " Index(['AAA', 'RAA', 'NAA', 'DAA', 'CAA', 'EAA', 'QAA', 'GAA', 'HAA', 'IAA',\n",
      "       ...\n",
      "       'KVV', 'MVV', 'FVV', 'PVV', 'SVV', 'TVV', 'WVV', 'YVV', 'VVV', 'Class'],\n",
      "      dtype='object', length=8001)\n",
      "\n",
      "Categorical features: []\n",
      "\n",
      "Columns with NaN:  0  /  8001\n",
      "\n",
      "No of data points with NaN: 0  /  1332\n",
      "Shape (1332, 8001)\n",
      "Shape X data: (1332, 8000)\n",
      "Shape Y data: (1332,)\n"
     ]
    }
   ],
   "source": [
    "sFile = r'C:\\Users\\Eric\\Documents\\GitHub\\machine-learning-for-druggable-proteins\\datasets\\ds.Class_TC_ballanced.csv'\n",
    "# get data from file\n",
    "Xdata, Ydata, Features = getDataFromDataset(sFile,outVar) # n_sample=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class weights =  {0: 1.0, 1: 1.0}\n"
     ]
    }
   ],
   "source": [
    "# Calculate class weights\n",
    "class_weights = set_weights(Ydata)\n",
    "print(\"Class weights = \", class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "outer_cv = StratifiedKFold(n_splits=3,shuffle=True,random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold = 1\n",
      "AUROC= 0.9620972323675026 ACC= 0.9234234234234234 1.2529455900192261 mins\n",
      "Fold = 2\n",
      "AUROC= 0.971370018667316 ACC= 0.9324324324324325 1.343106253941854 mins\n",
      "Fold = 3\n",
      "AUROC= 0.964572680788897 ACC= 0.9279279279279279 1.721213686466217 mins\n"
     ]
    }
   ],
   "source": [
    "ifold = 0\n",
    "ACCs  =[]\n",
    "AUROCs=[]\n",
    "models =[]\n",
    "SelectedFeatures =[]\n",
    "\n",
    "for train_index, test_index in outer_cv.split(Xdata, Ydata):\n",
    "    ifold +=1\n",
    "    \n",
    "    print(\"Fold =\",ifold)\n",
    "    start = time.time()\n",
    "    \n",
    "    #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_train, X_test = Xdata[train_index], Xdata[test_index]\n",
    "    y_train, y_test = Ydata[train_index], Ydata[test_index]\n",
    "    \n",
    "    # Standardize dataset\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test  = scaler.transform(X_test)\n",
    "    \n",
    "    # Feature selection # FS = SelectFromModel(LinearSVC(), max_features = 400,threshold=-np.inf)\n",
    "    lsvc = LinearSVC(max_iter=50000).fit(X_train, y_train)\n",
    "    model = SelectFromModel(lsvc, prefit=True,max_features = 200,threshold=-np.inf)\n",
    "    X_train = model.transform(X_train)\n",
    "    X_test  = model.transform(X_test)\n",
    "    #print(\"Selected X:\", X_train.shape)\n",
    "\n",
    "    # Selected features\n",
    "    SelFeatures = []\n",
    "    for i in model.get_support(indices=True):\n",
    "        SelFeatures.append(Features[i])\n",
    "    SelectedFeatures.append(SelFeatures)\n",
    "\n",
    "    #scaler.transform(X_test)\n",
    "    clf = SVC(kernel = 'rbf', random_state=seed,gamma='scale',\n",
    "              class_weight=class_weights,probability=True)\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    joblib.dump(clf, 'SVM_model'+str(ifold)+'.pkl', compress = 1)\n",
    "    models.append(clf)\n",
    "    \n",
    "    y_pred = clf.predict_proba(X_test)\n",
    "    AUROC = roc_auc_score(y_test, y_pred[:, 1])\n",
    "    AUROCs.append(AUROC)\n",
    "    \n",
    "    ACC = clf.score(X_test,y_test)\n",
    "    ACCs.append(ACC)\n",
    "   \n",
    "    print(\"AUROC=\",AUROC,\"ACC=\",ACC, (time.time() - start)/60,\"mins\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the mean AUROC values for best model and the standard deviations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9660133106079053 0.003920263779140678\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(AUROCs),np.std(AUROCs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9279279279279279 0.0036779125267014765\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(ACCs),np.std(ACCs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['NRA', 'INA', 'KCA', 'YEA', 'LGA', 'THA', 'IMA', 'YPA', 'CSA', 'ARR', 'KNR', 'WDR', 'TER', 'AQR', 'PQR', 'PPR', 'FWR', 'NYR', 'ERN', 'YRN', 'SDN', 'RHN', 'YIN', 'FWN', 'TWN', 'CDD', 'MED', 'LGD', 'SHD', 'PKD', 'WPD', 'SSD', 'NTD', 'HTD', 'DWD', 'EYD', 'VYD', 'QRC', 'YGC', 'NHC', 'IHC', 'VHC', 'EMC', 'GMC', 'GFC', 'QSC', 'GYC', 'EVC', 'DNE', 'GDE', 'KDE', 'NHE', 'ALE', 'RLE', 'HME', 'TSE', 'AWE', 'KWE', 'EYE', 'QYE', 'GVE', 'FVE', 'SAQ', 'TRQ', 'INQ', 'FNQ', 'PCQ', 'WEQ', 'RQQ', 'HLQ', 'RMQ', 'DFQ', 'HFQ', 'DSQ', 'YSQ', 'RVQ', 'NAG', 'HGG', 'QHG', 'NKG', 'RVG', 'TNH', 'FDH', 'PDH', 'TQH', 'DGH', 'CGH', 'KHH', 'KKH', 'WSH', 'FTH', 'FWH', 'CYH', 'TYH', 'FEI', 'WEI', 'WQI', 'QGI', 'DRL', 'IRL', 'PDL', 'NGL', 'IKL', 'DPL', 'GPL', 'ESL', 'VWL', 'DVL', 'MVL', 'QRK', 'GRK', 'HNK', 'YNK', 'MDK', 'HCK', 'DHK', 'QLK', 'MMK', 'SMK', 'FFK', 'EWK', 'TYK', 'WRM', 'WCM', 'REM', 'WQM', 'AGM', 'SHM', 'LLM', 'YLM', 'NFM', 'TPM', 'TSM', 'VTM', 'RWM', 'KYM', 'IVM', 'YVM', 'LDF', 'YQF', 'YKF', 'CFF', 'GFF', 'MTF', 'FWF', 'NAP', 'CNP', 'FNP', 'QGP', 'PLP', 'AKP', 'PPP', 'RSP', 'ITP', 'KWP', 'YWP', 'RVP', 'NAS', 'SRS', 'WDS', 'HCS', 'LES', 'IGS', 'DHS', 'SHS', 'SSS', 'GVS', 'TAT', 'DRT', 'GRT', 'IRT', 'PET', 'VQT', 'SIT', 'NLT', 'KKT', 'YTT', 'DWT', 'RCW', 'DEW', 'QGW', 'MIW', 'RFW', 'DFW', 'SFW', 'FYW', 'MVW', 'RRY', 'NRY', 'CCY', 'DMY', 'LMY', 'YPY', 'YAV', 'SRV', 'HNV', 'QGV', 'TGV', 'SKV', 'TSV'], ['WAA', 'NRA', 'INA', 'KCA', 'EEA', 'YEA', 'AQA', 'YIA', 'HLA', 'IMA', 'MMA', 'DSA', 'CSA', 'QTA', 'VYA', 'KNR', 'TER', 'PQR', 'EHR', 'VSR', 'MDN', 'DQN', 'LHN', 'YIN', 'RSN', 'FWN', 'HDD', 'ACD', 'SCD', 'MED', 'RGD', 'SHD', 'WLD', 'VLD', 'VFD', 'WPD', 'LTD', 'DWD', 'QWD', 'YWD', 'VYD', 'WRC', 'NDC', 'IHC', 'WHC', 'VHC', 'EMC', 'FWC', 'SDE', 'NHE', 'HME', 'TSE', 'EYE', 'QYE', 'GVE', 'ENQ', 'INQ', 'NDQ', 'WEQ', 'RQQ', 'KQQ', 'NGQ', 'CHQ', 'HLQ', 'GPQ', 'YSQ', 'QYQ', 'RVQ', 'QRG', 'GGG', 'HGG', 'KGG', 'DHG', 'GIG', 'SFG', 'SSG', 'RTG', 'PTG', 'AWG', 'IVG', 'CDH', 'PDH', 'YDH', 'DGH', 'KHH', 'QFH', 'KPH', 'FWH', 'LVH', 'PAI', 'WRI', 'QNI', 'WEI', 'WQI', 'NII', 'PMI', 'SPI', 'ESI', 'QTI', 'GTI', 'AYI', 'PDL', 'EKL', 'IKL', 'DPL', 'GPL', 'ESL', 'DVL', 'MVL', 'YVL', 'VVL', 'HNK', 'AEK', 'QLK', 'SMK', 'FFK', 'TYK', 'WRM', 'WNM', 'CDM', 'REM', 'FEM', 'WQM', 'SHM', 'LLM', 'SMM', 'RWM', 'GYM', 'HVM', 'IVM', 'LDF', 'YQF', 'VGF', 'AIF', 'YKF', 'MMF', 'AFF', 'CFF', 'FWF', 'TYF', 'FAP', 'FNP', 'QGP', 'VHP', 'PIP', 'PLP', 'HKP', 'RSP', 'ETP', 'KWP', 'YWP', 'LYP', 'SRS', 'CDS', 'HDS', 'HCS', 'LES', 'WGS', 'DHS', 'SHS', 'PSS', 'QAT', 'DRT', 'GRT', 'RNT', 'INT', 'PET', 'VQT', 'NLT', 'DLT', 'WLT', 'KKT', 'QWT', 'FYT', 'CRW', 'RCW', 'KCW', 'GEW', 'RGW', 'VGW', 'DIW', 'GKW', 'DFW', 'NRY', 'TRY', 'YRY', 'TGY', 'LMY', 'AFY', 'YAV', 'SRV', 'ENV', 'HNV', 'QGV', 'TGV', 'WHV', 'LLV', 'IMV', 'HSV', 'HTV'], ['NRA', 'YEA', 'DGA', 'CGA', 'VIA', 'CSA', 'PYA', 'VYA', 'KNR', 'TER', 'AQR', 'PQR', 'EHR', 'HIR', 'LIR', 'TFR', 'PPR', 'PVR', 'CNN', 'MDN', 'YIN', 'TKN', 'HFN', 'TWN', 'RCD', 'MED', 'LGD', 'AHD', 'CHD', 'SHD', 'PKD', 'DFD', 'PPD', 'WPD', 'SSD', 'VYD', 'NDC', 'IDC', 'NHC', 'IHC', 'VHC', 'QKC', 'NPC', 'ETC', 'HTC', 'GYC', 'MCE', 'NHE', 'HME', 'PME', 'AWE', 'NWE', 'EYE', 'QYE', 'SAQ', 'HRQ', 'EDQ', 'MDQ', 'PCQ', 'NEQ', 'WEQ', 'RQQ', 'NGQ', 'HGQ', 'HLQ', 'AKQ', 'VKQ', 'PPQ', 'DSQ', 'MVQ', 'QRG', 'CDG', 'HGG', 'TGG', 'WMG', 'FPG', 'ISG', 'RTG', 'IVG', 'YAH', 'VRH', 'FDH', 'PDH', 'SDH', 'YDH', 'TQH', 'DGH', 'CGH', 'KHH', 'GMH', 'YMH', 'HTH', 'CYH', 'LVH', 'NDI', 'SCI', 'WEI', 'PMI', 'DWI', 'EVI', 'QDL', 'EQL', 'EKL', 'HKL', 'KFL', 'GPL', 'DWL', 'WYL', 'DVL', 'MVL', 'VVL', 'GNK', 'HCK', 'EQK', 'RGK', 'QLK', 'SMK', 'VYK', 'WRM', 'WNM', 'MDM', 'REM', 'NEM', 'FEM', 'WQM', 'DHM', 'SHM', 'NFM', 'TSM', 'GWM', 'IVM', 'YVM', 'HAF', 'LDF', 'SEF', 'AFF', 'CNP', 'QNP', 'NDP', 'PEP', 'QGP', 'AKP', 'GSP', 'TTP', 'KWP', 'YWP', 'SAS', 'SRS', 'VRS', 'WDS', 'HCS', 'LES', 'IGS', 'DHS', 'SHS', 'SPS', 'ASS', 'LWS', 'WWS', 'LAT', 'DRT', 'GRT', 'INT', 'FNT', 'VQT', 'WGT', 'YHT', 'CPT', 'QWT', 'GYT', 'RGW', 'QGW', 'MIW', 'GKW', 'IKW', 'RFW', 'DFW', 'YPW', 'MSW', 'RWW', 'QYW', 'KVW', 'NRY', 'AQY', 'LGY', 'TGY', 'KPY', 'YPY', 'WSY', 'SRV', 'HNV', 'ACV', 'GEV', 'QGV', 'HGV', 'LLV', 'SKV', 'GMV', 'SPV', 'QYV']]\n"
     ]
    }
   ],
   "source": [
    "# all the selected features for the 3 folds\n",
    "print(SelectedFeatures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ETP', 'NDQ', 'HSV', 'WNM', 'KQQ', 'SCD', 'QNI', 'LVH', 'EKL', 'YVL', 'VGF', 'YWD', 'ACD', 'VGW', 'SDE', 'VSR', 'CDM', 'NDC', 'AWG', 'QTI', 'IMV', 'FAP', 'DIW', 'TGY', 'SFG', 'SPI', 'TRY', 'GPQ', 'QFH', 'WHV', 'VFD', 'QWD', 'MDN', 'LHN', 'WAA', 'FWC', 'HDD', 'AFY', 'WLD', 'WHC', 'INT', 'VYA', 'DQN', 'HTV', 'QWT', 'ESI', 'PMI', 'ENV', 'GYM', 'RGD', 'RNT', 'HKP', 'IVG', 'CDH', 'VHP', 'RGW', 'YIA', 'CRW', 'EEA', 'NII', 'HVM', 'KPH', 'RTG', 'WGS', 'GKW', 'VVL', 'FYT', 'TYF', 'EHR', 'PIP', 'GEW', 'FEM', 'MMA', 'KCW', 'AQA', 'GTI', 'CHQ', 'WLT', 'LLV', 'YRY', 'LYP', 'PSS', 'KGG', 'AEK', 'AYI', 'GIG', 'CDS', 'ENQ', 'RSN', 'QRG', 'QYQ', 'DLT', 'AIF', 'PTG', 'PAI', 'SMM', 'QTA', 'GGG', 'HDS', 'YDH', 'VLD', 'SSG', 'WRC', 'DSA', 'NGQ', 'QAT', 'LTD', 'WRI', 'DHG', 'MMF', 'AFF', 'HLA']\n"
     ]
    }
   ],
   "source": [
    "# differences of selected descriptors\n",
    "print(list(set(SelectedFeatures[1])-set(SelectedFeatures[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ETP', 'FNP', 'NDQ', 'HSV', 'CFF', 'KQQ', 'SCD', 'QNI', 'YVL', 'VGF', 'YWD', 'LLM', 'ACD', 'VGW', 'SDE', 'VSR', 'KKT', 'CDM', 'QTI', 'AWG', 'IMV', 'FAP', 'DIW', 'YSQ', 'PDL', 'SFG', 'SPI', 'TRY', 'NLT', 'GPQ', 'QFH', 'WHV', 'VFD', 'QWD', 'LHN', 'HNK', 'WAA', 'FWC', 'HDD', 'AFY', 'YAV', 'WLD', 'WHC', 'PET', 'DQN', 'HTV', 'FFK', 'FWF', 'ESI', 'ENV', 'TYK', 'GYM', 'RGD', 'RNT', 'HKP', 'RVQ', 'WQI', 'CDH', 'VHP', 'LMY', 'YIA', 'INQ', 'CRW', 'EEA', 'NII', 'HVM', 'YKF', 'KPH', 'RWM', 'WGS', 'DWD', 'TYF', 'FYT', 'TGV', 'PIP', 'INA', 'GEW', 'KCA', 'TSE', 'MMA', 'KCW', 'AQA', 'GTI', 'CHQ', 'WLT', 'EMC', 'YRY', 'LYP', 'PSS', 'KGG', 'AEK', 'GVE', 'AYI', 'GIG', 'RSP', 'CDS', 'ENQ', 'PLP', 'RSN', 'YQF', 'QYQ', 'DLT', 'ESL', 'AIF', 'PTG', 'PAI', 'SMM', 'QTA', 'GGG', 'HDS', 'DPL', 'IMA', 'VLD', 'SSG', 'RCW', 'WRC', 'DSA', 'FWH', 'QAT', 'FWN', 'LTD', 'WRI', 'DHG', 'MMF', 'IKL', 'HLA']\n"
     ]
    }
   ],
   "source": [
    "# differences of selected descriptors\n",
    "print(list(set(SelectedFeatures[1])-set(SelectedFeatures[2])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions with the best model\n",
    "\n",
    "We choose model 2 as the best due to the maximum ACC value (AUROC= 0.9752, ACC= 0.937)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the prediction datasets (the same format as the dataset: 8000 TC features + Class=-1):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-> Read dataset C:\\Users\\Eric\\Documents\\GitHub\\machine-learning-for-druggable-proteins\\cdca3.csv\n",
      "\n",
      "Data points = 1\n",
      "\n",
      "Columns (output + features)= 8001\n",
      "\n",
      "Data types = [dtype('float64')]\n",
      "\n",
      "\n",
      "Column Names:\n",
      " Index(['AAA', 'RAA', 'NAA', 'DAA', 'CAA', 'EAA', 'QAA', 'GAA', 'HAA', 'IAA',\n",
      "       ...\n",
      "       'KVV', 'MVV', 'FVV', 'PVV', 'SVV', 'TVV', 'WVV', 'YVV', 'VVV', 'Class'],\n",
      "      dtype='object', length=8001)\n",
      "\n",
      "Categorical features: []\n",
      "\n",
      "Columns with NaN:  0  /  8001\n",
      "\n",
      "No of data points with NaN: 0  /  1\n",
      "Shape (1, 8001)\n",
      "Shape X data: (1, 8000)\n",
      "Shape Y data: (1,)\n",
      "(1, 8000)\n"
     ]
    }
   ],
   "source": [
    "cdca3 = \"\"\"MGSAKSVPVTPARPPPHNKHLARVADPRSPSAGILRTPIQVESSPQPGLPAGEQLEGLKH\n",
    "AQDSDPRSPTLGIARTPMKTSSGDPPSPLVKQLSEVFETEDSKSNLPPEPVLPPEAPLSS\n",
    "ELDLPLGTQLSVEEQMPPWNQTEFPSKQVFSKEEARQPTETPVASQSSDKPSRDPETPRS\n",
    "SGSMRNRWKPNSSKVLGRSPLTILQDDNSPGTLTLRQGKRPSPLSENVSELKEGAILGTG\n",
    "RLLKTGGRAWEQGQDHDKENQHFPLVES\"\"\"\n",
    "\n",
    "header = open(\"example_header.csv\", \"r\").readlines()[0].split(\",\")\n",
    "\n",
    "\n",
    "counts = []\n",
    "\n",
    "\n",
    "for x in header:\n",
    "    count = cdca3.count(x)\n",
    "    counts.append(count)\n",
    "\n",
    "fractions = [c/len(counts[:-1]) for c in counts]\n",
    "\n",
    "fractions[-1] = -1\n",
    "    \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "df = pd.DataFrame(np.array([fractions]), columns=header)\n",
    "\n",
    "df.to_csv(\"cdca3.csv\", index=False)\n",
    "\n",
    "\n",
    "\n",
    "# get data from files and check the files\n",
    "sFile1 = r'C:\\Users\\Eric\\Documents\\GitHub\\machine-learning-for-druggable-proteins\\cdca3.csv'\n",
    "Xdata1, Ydata1, Features1 = getDataFromDataset(sFile1,outVar) \n",
    "\n",
    "print(Xdata1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use only the second split / model - scale the prediction datasets, select only the features of model 2, predict the class and predict the probability of that class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold = 2\n",
      "Xdata1 sel= (1, 200)\n",
      "Time 1.0893101930618285 mins\n"
     ]
    }
   ],
   "source": [
    "ifold = 0\n",
    "\n",
    "for train_index, test_index in outer_cv.split(Xdata, Ydata):\n",
    "    ifold +=1\n",
    "    \n",
    "    if ifold ==2: # only model 2\n",
    "        print(\"Fold =\",ifold)\n",
    "        start = time.time()\n",
    "\n",
    "        #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "        X_train, X_test = Xdata[train_index], Xdata[test_index]\n",
    "        y_train, y_test = Ydata[train_index], Ydata[test_index]\n",
    "\n",
    "        # Standardize dataset\n",
    "        scaler = StandardScaler()\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        X_test  = scaler.transform(X_test)\n",
    "        # scale prediction set\n",
    "        Xdata1  = scaler.transform(Xdata1)\n",
    "\n",
    "        \n",
    "        # Feature selection # FS = SelectFromModel(LinearSVC(), max_features = 400,threshold=-np.inf)\n",
    "        lsvc = LinearSVC(max_iter=50000).fit(X_train, y_train)\n",
    "        model = SelectFromModel(lsvc, prefit=True,max_features = 200,threshold=-np.inf)\n",
    "        X_train = model.transform(X_train)\n",
    "        X_test  = model.transform(X_test)\n",
    "    \n",
    "        # Selected features\n",
    "        SelFeatures = []\n",
    "        for i in model.get_support(indices=True):\n",
    "            SelFeatures.append(Features[i])\n",
    "        \n",
    "        # apply selected features to prediction set\n",
    "        Xdata1 = Xdata1[:, model.get_support()]\n",
    "        print(\"Xdata1 sel=\", Xdata1.shape)\n",
    "\n",
    "\n",
    "        # we dont need to calculate again, but load the model from the disk!\n",
    "        #clf = SVC(kernel = 'rbf', random_state=seed,gamma='scale',\n",
    "        #          class_weight=class_weights,probability=True)\n",
    "        #clf.fit(X_train, y_train)\n",
    "\n",
    "        # load the saved model from disk\n",
    "        clf = joblib.load('SVM_model'+str(ifold)+'.pkl')\n",
    "        #joblib.dump(clf, 'SVM_model'+str(ifold)+'.pkl', compress = 1)\n",
    "        \n",
    "        # predictions with the model\n",
    "        Ydata1 = clf.predict(Xdata1)\n",
    "\n",
    "        # add probabilities (n_samples X n_classes; class 0, class 1)\n",
    "        Ydata1prob = clf.predict_proba(Xdata1)\n",
    "\n",
    "        # save predictions for list 1\n",
    "        dff1 = pd.DataFrame(Xdata1,columns=SelFeatures)\n",
    "        dff1['Class'] = Ydata1\n",
    "        dff1['Prob0'] = Ydata1prob[:,0]\n",
    "        dff1['Prob1'] = Ydata1prob[:,1]\n",
    "        \n",
    "        # merge with protein information from other file\n",
    "        #result = pd.concat([dff1, pd.read_csv('./PREDICTIONS/TC_seqs.Screening_1_Cancer_Driver_Genes.csv')], axis=1)\n",
    "        # creat new order of columns in final results\n",
    "       # newHeader=['Class','Prob1','Prob0','V1','V2']\n",
    "        #result = result[newHeader]\n",
    "       # result = result.sort_values(by=['Prob1'], ascending=False)\n",
    "        dff1.to_csv(sFile1[:-4]+'_predictions.csv', index=True)\n",
    "\n",
    "\n",
    "        print(\"Time\",(time.time() - start)/60,\"mins\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Chekc the results:\n",
      "./datasets/ds.Screening_1_TC_predictions.csv\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'sFile2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-08359f1a4b7c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"==> Chekc the results:\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msFile1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'_predictions.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msFile2\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'_predictions.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msFile3\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'_predictions.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'sFile2' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"==> Chekc the results:\")\n",
    "print(sFile1[:-4]+'_predictions.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hf with ML!@muntisa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
