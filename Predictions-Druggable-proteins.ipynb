{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictions of Druggable peptides using the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "# remove warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import confusion_matrix,accuracy_score, roc_auc_score,f1_score, recall_score, precision_score\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression, LassoCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "from sklearn.feature_selection import RFECV, VarianceThreshold, SelectKBest, chi2\n",
    "from sklearn.feature_selection import SelectFromModel, SelectPercentile, f_classif\n",
    "\n",
    "import seaborn as sns; sns.set() # data visualization library \n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier, BaggingClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB, GaussianNB\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score, KFold, StratifiedKFold\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "print(__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DataCheckings(df):\n",
    "    # CHECKINGS ***************************\n",
    "    # Check the number of data points in the data set\n",
    "    print(\"\\nData points =\", len(df))\n",
    "    \n",
    "    # Check the number of columns in the data set\n",
    "    print(\"\\nColumns (output + features)=\",len(df.columns))\n",
    "    \n",
    "    # Check the data types\n",
    "    print(\"\\nData types =\", df.dtypes.unique())\n",
    "    \n",
    "    # Dataset statistics\n",
    "    print('\\n')\n",
    "    df.describe()\n",
    "    \n",
    "    # print names of columns\n",
    "    print('Column Names:\\n', df.columns)\n",
    "    \n",
    "    # see if there are categorical data\n",
    "    print(\"\\nCategorical features:\", df.select_dtypes(include=['O']).columns.tolist())\n",
    "    \n",
    "    # Check NA values\n",
    "    # Check any number of columns with NaN\n",
    "    print(\"\\nColumns with NaN: \", df.isnull().any().sum(), ' / ', len(df.columns))\n",
    "\n",
    "    # Check any number of data points with NaN\n",
    "    print(\"\\nNo of data points with NaN:\", df.isnull().any(axis=1).sum(), ' / ', len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDataFromDataset(sFile, OutVar):\n",
    "    # read details file\n",
    "    print('\\n-> Read dataset', sFile)\n",
    "    df = pd.read_csv(sFile)\n",
    "    #df = feather.read_dataframe(sFile)\n",
    "    \n",
    "    DataCheckings(df)\n",
    "    \n",
    "    # remove duplicates!\n",
    "    df.drop_duplicates(keep=False, inplace=True)\n",
    "    \n",
    "    print('Shape', df.shape)\n",
    "    # print(list(df.columns))\n",
    "\n",
    "    # select X and Y\n",
    "    ds_y = df[OutVar]\n",
    "    ds_X = df.drop(OutVar,axis = 1)\n",
    "    Xdata = ds_X.values # get values of features\n",
    "    Ydata = ds_y.values # get output values\n",
    "\n",
    "    print('Shape X data:', Xdata.shape)\n",
    "    print('Shape Y data:',Ydata.shape)\n",
    "    \n",
    "    # return data for X and Y, feature names as list\n",
    "    return (Xdata, Ydata, list(ds_X.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def  set_weights(y_data, option='balanced'):\n",
    "    \"\"\"Estimate class weights for umbalanced dataset\n",
    "       If ‘balanced’, class weights will be given by n_samples / (n_classes * np.bincount(y)). \n",
    "       If a dictionary is given, keys are classes and values are corresponding class weights. \n",
    "       If None is given, the class weights will be uniform \"\"\"\n",
    "    cw = class_weight.compute_class_weight(option, np.unique(y_data), y_data)\n",
    "    w = {i:j for i,j in zip(np.unique(y_data), cw)}\n",
    "    return w "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define output variables\n",
    "outVar = 'Class'\n",
    "\n",
    "# define list of folds\n",
    "foldType = 3\n",
    "\n",
    "# define a label for output files\n",
    "targetName = 'GS_Outer'\n",
    "\n",
    "seed = 28"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reproduce the pipeline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sFile = './datasets/ds.Class_TC_ballanced.csv'\n",
    "\n",
    "# get data from file\n",
    "Xdata, Ydata, Features = getDataFromDataset(sFile,outVar) # n_sample=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate class weights\n",
    "class_weights = set_weights(Ydata)\n",
    "print(\"Class weights = \", class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outer_cv = StratifiedKFold(n_splits=3,shuffle=True,random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ifold = 0\n",
    "ACCs  =[]\n",
    "AUROCs=[]\n",
    "models =[]\n",
    "SelectedFeatures =[]\n",
    "\n",
    "for train_index, test_index in outer_cv.split(Xdata, Ydata):\n",
    "    ifold +=1\n",
    "    \n",
    "    print(\"Fold =\",ifold)\n",
    "    start = time.time()\n",
    "    \n",
    "    #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_train, X_test = Xdata[train_index], Xdata[test_index]\n",
    "    y_train, y_test = Ydata[train_index], Ydata[test_index]\n",
    "    \n",
    "    # Standardize dataset\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test  = scaler.transform(X_test)\n",
    "    \n",
    "    # Feature selection # FS = SelectFromModel(LinearSVC(), max_features = 400,threshold=-np.inf)\n",
    "    lsvc = LinearSVC(max_iter=50000).fit(X_train, y_train)\n",
    "    model = SelectFromModel(lsvc, prefit=True,max_features = 200,threshold=-np.inf)\n",
    "    X_train = model.transform(X_train)\n",
    "    X_test  = model.transform(X_test)\n",
    "    #print(\"Selected X:\", X_train.shape)\n",
    "\n",
    "    # Selected features\n",
    "    SelFeatures = []\n",
    "    for i in model.get_support(indices=True):\n",
    "        SelFeatures.append(Features[i])\n",
    "    SelectedFeatures.append(SelFeatures)\n",
    "\n",
    "    #scaler.transform(X_test)\n",
    "    clf = SVC(kernel = 'rbf', random_state=seed,gamma='scale',\n",
    "              class_weight=class_weights,probability=True)\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    joblib.dump(clf, 'SVM_model'+str(ifold)+'.pkl', compress = 1)\n",
    "    models.append(clf)\n",
    "    \n",
    "    y_pred = clf.predict_proba(X_test)\n",
    "    AUROC = roc_auc_score(y_test, y_pred[:, 1])\n",
    "    AUROCs.append(AUROC)\n",
    "    \n",
    "    ACC = clf.score(X_test,y_test)\n",
    "    ACCs.append(ACC)\n",
    "   \n",
    "    print(\"AUROC=\",AUROC,\"ACC=\",ACC, (time.time() - start)/60,\"mins\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the mean AUROC values for best model and the standard deviations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(AUROCs),np.std(AUROCs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(ACCs),np.std(ACCs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all the selected features for the 3 folds\n",
    "print(SelectedFeatures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# differences of selected descriptors\n",
    "print(list(set(SelectedFeatures[1])-set(SelectedFeatures[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# differences of selected descriptors\n",
    "print(list(set(SelectedFeatures[1])-set(SelectedFeatures[2])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions with the best model\n",
    "\n",
    "We choose model 2 as the best due to the maximum ACC value (AUROC= 0.9752, ACC= 0.937)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the selected features for model 2\n",
    "print(SelectedFeatures[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the prediction datasets (the same format as the dataset: 8000 TC features + Class=-1):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get data from files and check the files\n",
    "sFile1 = './datasets/ds.Screening_1_TC.csv'\n",
    "Xdata1, Ydata1, Features1 = getDataFromDataset(sFile1,outVar) \n",
    "\n",
    "sFile2 = './datasets/ds.Screening_2_TC.csv'\n",
    "Xdata2, Ydata2, Features2 = getDataFromDataset(sFile2,outVar) \n",
    "\n",
    "sFile3 = './datasets/ds.Screening_3_TC.csv'\n",
    "Xdata3, Ydata3, Features3 = getDataFromDataset(sFile3,outVar)\n",
    "\n",
    "print(Xdata2.shape,Xdata3.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use only the second split / model - scale the prediction datasets, select only the features of model 2, predict the class and predict the probability of that class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ifold = 0\n",
    "\n",
    "for train_index, test_index in outer_cv.split(Xdata, Ydata):\n",
    "    ifold +=1\n",
    "    \n",
    "    if ifold ==2: # only model 2\n",
    "        print(\"Fold =\",ifold)\n",
    "        start = time.time()\n",
    "\n",
    "        #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "        X_train, X_test = Xdata[train_index], Xdata[test_index]\n",
    "        y_train, y_test = Ydata[train_index], Ydata[test_index]\n",
    "\n",
    "        # Standardize dataset\n",
    "        scaler = StandardScaler()\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        X_test  = scaler.transform(X_test)\n",
    "        # scale prediction set\n",
    "        Xdata1  = scaler.transform(Xdata1)\n",
    "        Xdata2  = scaler.transform(Xdata2)\n",
    "        Xdata3  = scaler.transform(Xdata3)\n",
    "        \n",
    "        # Feature selection # FS = SelectFromModel(LinearSVC(), max_features = 400,threshold=-np.inf)\n",
    "        lsvc = LinearSVC(max_iter=50000).fit(X_train, y_train)\n",
    "        model = SelectFromModel(lsvc, prefit=True,max_features = 200,threshold=-np.inf)\n",
    "        X_train = model.transform(X_train)\n",
    "        X_test  = model.transform(X_test)\n",
    "    \n",
    "        # Selected features\n",
    "        SelFeatures = []\n",
    "        for i in model.get_support(indices=True):\n",
    "            SelFeatures.append(Features[i])\n",
    "        \n",
    "        # apply selected features to prediction set\n",
    "        Xdata1 = Xdata1[:, model.get_support()]\n",
    "        print(\"Xdata1 sel=\", Xdata1.shape)\n",
    "        Xdata2 = Xdata2[:, model.get_support()]\n",
    "        print(\"Xdata2 sel=\", Xdata2.shape)\n",
    "        Xdata3 = Xdata3[:, model.get_support()]\n",
    "        print(\"Xdata3 sel=\", Xdata3.shape)\n",
    "\n",
    "        # we dont need to calculate again, but load the model from the disk!\n",
    "        #clf = SVC(kernel = 'rbf', random_state=seed,gamma='scale',\n",
    "        #          class_weight=class_weights,probability=True)\n",
    "        #clf.fit(X_train, y_train)\n",
    "\n",
    "        # load the saved model from disk\n",
    "        clf = joblib.load('SVM_model'+str(ifold)+'.pkl')\n",
    "        #joblib.dump(clf, 'SVM_model'+str(ifold)+'.pkl', compress = 1)\n",
    "        \n",
    "        # predictions with the model\n",
    "        Ydata1 = clf.predict(Xdata1)\n",
    "        Ydata2 = clf.predict(Xdata2)\n",
    "        Ydata3 = clf.predict(Xdata3)\n",
    "        \n",
    "        # add probabilities (n_samples X n_classes; class 0, class 1)\n",
    "        Ydata1prob = clf.predict_proba(Xdata1)\n",
    "        Ydata2prob = clf.predict_proba(Xdata2)\n",
    "        Ydata3prob = clf.predict_proba(Xdata3)\n",
    "        \n",
    "        # save predictions for list 1\n",
    "        dff1 = pd.DataFrame(Xdata1,columns=SelFeatures)\n",
    "        dff1['Class'] = Ydata1\n",
    "        dff1['Prob0'] = Ydata1prob[:,0]\n",
    "        dff1['Prob1'] = Ydata1prob[:,1]\n",
    "        # merge with protein information from other file\n",
    "        result = pd.concat([dff1, pd.read_csv('./PREDICTIONS/TC_seqs.Screening_1_Cancer_Driver_Genes.csv')], axis=1)\n",
    "        # creat new order of columns in final results\n",
    "        newHeader=['Class','Prob1','Prob0','V1','V2']\n",
    "        result = result[newHeader]\n",
    "        result = result.sort_values(by=['Prob1'], ascending=False)\n",
    "        result.to_csv(sFile1[:-4]+'_predictions.csv', index=True)\n",
    "\n",
    "        # save predictions for list 2\n",
    "        dff2 = pd.DataFrame(Xdata2,columns=SelFeatures)\n",
    "        dff2['Class'] = Ydata2\n",
    "        dff2['Prob0'] = Ydata2prob[:,0]\n",
    "        dff2['Prob1'] = Ydata2prob[:,1]\n",
    "        # merge with protein information from other file\n",
    "        result = pd.concat([dff2, pd.read_csv('./PREDICTIONS/TC_seqs.Screening_2_OncoOmics_Genes.csv')], axis=1)\n",
    "        # creat new order of columns in final results\n",
    "        newHeader=['Class','Prob1','Prob0','V1','V2']\n",
    "        result = result[newHeader]\n",
    "        result = result.sort_values(by=['Prob1'], ascending=False)\n",
    "        result.to_csv(sFile2[:-4]+'_predictions.csv', index=True)\n",
    "        \n",
    "        # save predictions for list 3\n",
    "        dff3 = pd.DataFrame(Xdata3,columns=SelFeatures)\n",
    "        dff3['Class'] = Ydata3\n",
    "        dff3['Prob0'] = Ydata3prob[:,0]\n",
    "        dff3['Prob1'] = Ydata3prob[:,1]\n",
    "        # merge with protein information from other file\n",
    "        result = pd.concat([dff3, pd.read_csv('./PREDICTIONS/TC_seqs.Screening_3_RBPs.csv')], axis=1)\n",
    "        # creat new order of columns in final results\n",
    "        newHeader=['Class','Prob1','Prob0','V1','V2']\n",
    "        result = result[newHeader]\n",
    "        result = result.sort_values(by=['Prob1'], ascending=False)\n",
    "        result.to_csv(sFile3[:-4]+'_predictions.csv', index=True)\n",
    "\n",
    "        print(\"Time\",(time.time() - start)/60,\"mins\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"==> Chekc the results:\")\n",
    "print(sFile1[:-4]+'_predictions.csv')\n",
    "print(sFile2[:-4]+'_predictions.csv')\n",
    "print(sFile3[:-4]+'_predictions.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hf with ML!@muntisa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
